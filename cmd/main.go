package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"time"

	"action-perfect-get-on-go/pkg/cleaner"
	"action-perfect-get-on-go/pkg/scraper"
	"action-perfect-get-on-go/pkg/types"

	"github.com/spf13/cobra"
)

// ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
var llmAPIKey string
var llmTimeout time.Duration
var scraperTimeout time.Duration

func init() {
	rootCmd.PersistentFlags().DurationVarP(&llmTimeout, "llm-timeout", "t", 5*time.Minute, "LLMå‡¦ç†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“")
	rootCmd.PersistentFlags().DurationVarP(&scraperTimeout, "scraper-timeout", "s", 15*time.Second, "Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®HTTPã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“")
	rootCmd.PersistentFlags().StringVarP(&llmAPIKey, "api-key", "k", "", "Gemini APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ãŒå„ªå…ˆ)")
}

// ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
func main() {
	if err := rootCmd.Execute(); err != nil {
		os.Exit(1)
	}
}

var rootCmd = &cobra.Command{
	Use:   "action-perfect-get-on-go [URL...]",
	Short: "è¤‡æ•°ã®URLã‚’ä¸¦åˆ—ã§å–å¾—ã—ã€LLMã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚",
	Long: `
Action Perfect Get On Ready to Go
éŠ€æ²³ã®æœã¦ã¾ã§ è¿½ã„ã‹ã‘ã¦ã‚†ã é­‚ã®è¡€æ½®ã§ ã‚¢ã‚¯ã‚»ãƒ«è¸ã¿è¾¼ã¿

è¤‡æ•°ã®URLã‚’ä¸¦åˆ—ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€å–å¾—ã—ãŸæœ¬æ–‡ã‚’LLMã§é‡è¤‡æ’é™¤ãƒ»æ§‹é€ åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
[URL...]ã¨ã—ã¦ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¤‡æ•°ã®URLã‚’å¼•æ•°ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
`,
	Args: cobra.MinimumNArgs(1),
	RunE: runMain,
}

// runMain ã¯ CLIã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
func runMain(cmd *cobra.Command, args []string) error {
	urls := args

	// LLMå‡¦ç†ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ãƒ•ãƒ©ã‚°å€¤ã§è¨­å®š
	ctx, cancel := context.WithTimeout(cmd.Context(), llmTimeout)
	defer cancel()

	log.Printf("ğŸš€ Action Perfect Get On: %då€‹ã®URLã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚", len(urls))

	// --- 1. ä¸¦åˆ—æŠ½å‡ºãƒ•ã‚§ãƒ¼ã‚º (Scraping) ---
	log.Println("--- 1. Webã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸¦åˆ—æŠ½å‡ºã‚’é–‹å§‹ ---")

	// ParallelScraperã®åˆæœŸåŒ– (ã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯)
	s, err := scraper.NewParallelScraper(scraperTimeout)
	if err != nil {
		return fmt.Errorf("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	// ä¸¦åˆ—å®Ÿè¡Œ
	results := s.ScrapeInParallel(ctx, urls)

	// 1. ç„¡æ¡ä»¶é…å»¶
	log.Println("ä¸¦åˆ—æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è€ƒæ…®ã—ã€æ¬¡ã®å‡¦ç†ã«é€²ã‚€å‰ã«1ç§’å¾…æ©Ÿã—ã¾ã™ã€‚")
	time.Sleep(1 * time.Second)

	// 2. çµæœã®åˆ†é¡
	var successfulResults []types.URLResult
	var failedURLs []string

	for _, res := range results {
		// ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã€ã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã®å ´åˆã¯å¤±æ•—ã¨è¦‹ãªã™
		if res.Error != nil || res.Content == "" {
			log.Printf("âŒ ERROR: %s ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: %v", res.URL, res.Error)
			failedURLs = append(failedURLs, res.URL)
		} else {
			successfulResults = append(successfulResults, res)
		}
	}

	// 3. å¤±æ•—URLãŒã‚ã‚‹å ´åˆã€è¿½åŠ ã§5ç§’å¾…æ©Ÿå¾Œã«é †æ¬¡ãƒªãƒˆãƒ©ã‚¤
	if len(failedURLs) > 0 {
		log.Printf("âš ï¸ WARNING: æŠ½å‡ºã«å¤±æ•—ã—ãŸURLãŒ %d ä»¶ã‚ã‚Šã¾ã™ã€‚5ç§’å¾…æ©Ÿå¾Œã€é †æ¬¡ãƒªãƒˆãƒ©ã‚¤ã‚’é–‹å§‹ã—ã¾ã™ã€‚", len(failedURLs))
		time.Sleep(5 * time.Second) // ãƒªãƒˆãƒ©ã‚¤å‰ã®è¿½åŠ é…å»¶

		// ãƒªãƒˆãƒ©ã‚¤ç”¨ã®éä¸¦åˆ—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
		retryScraperClient, err := scraper.NewClient(scraperTimeout)
		if err != nil {
			log.Printf("ERROR: ãƒªãƒˆãƒ©ã‚¤ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %v", err)
		} else {
			log.Println("--- 1b. å¤±æ•—URLã®é †æ¬¡ãƒªãƒˆãƒ©ã‚¤ã‚’é–‹å§‹ ---")

			for _, url := range failedURLs {
				log.Printf("ãƒªãƒˆãƒ©ã‚¤ä¸­: %s", url)

				// é †æ¬¡å†è©¦è¡Œ (éä¸¦åˆ—)
				content, err := retryScraperClient.ExtractContent(url, ctx)

				if err != nil || content == "" {
					log.Printf("âŒ ERROR: ãƒªãƒˆãƒ©ã‚¤ã§ã‚‚ %s ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: %v", url, err)
				} else {
					log.Printf("âœ… SUCCESS: %s ã®æŠ½å‡ºãŒãƒªãƒˆãƒ©ã‚¤ã§æˆåŠŸã—ã¾ã—ãŸã€‚", url)
					// ãƒªãƒˆãƒ©ã‚¤ã§æˆåŠŸã—ãŸã‚‚ã®ã‚’æˆåŠŸãƒªã‚¹ãƒˆã«è¿½åŠ 
					successfulResults = append(successfulResults, types.URLResult{
						URL:     url,
						Content: content,
						Error:   nil,
					})
				}
			}
		}
	}

	// æˆåŠŸURLãŒã‚¼ãƒ­ã®å ´åˆã¯çµ‚äº†
	if len(successfulResults) == 0 {
		return fmt.Errorf("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: æœ€çµ‚çš„ã«å‡¦ç†å¯èƒ½ãªWebã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
	}

	// --- 2. ãƒ‡ãƒ¼ã‚¿çµåˆãƒ•ã‚§ãƒ¼ã‚º (ãƒªãƒˆãƒ©ã‚¤æˆåŠŸçµæœã‚‚å«ã‚€) ---
	log.Println("--- 2. æŠ½å‡ºçµæœã®çµåˆ ---")

	combinedText := cleaner.CombineContents(successfulResults)

	log.Printf("çµåˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•: %dãƒã‚¤ãƒˆ (æˆåŠŸ: %d/%d URL)",
		len(combinedText), len(successfulResults), len(urls))

	// --- 3. AIã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ•ã‚§ãƒ¼ã‚º (LLM) ---
	log.Println("--- 3. LLMã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨æ§‹é€ åŒ–ã‚’é–‹å§‹ (Go-AI-Clientåˆ©ç”¨) ---")

	cleanedText, err := cleaner.CleanAndStructureText(ctx, combinedText, llmAPIKey)
	if err != nil {
		return fmt.Errorf("LLMã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	// --- 4. æœ€çµ‚çµæœã®å‡ºåŠ› ---
	fmt.Println("\n===============================================")
	fmt.Println("âœ… PERFECT GET ON: LLMã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®æœ€çµ‚å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿:")
	fmt.Println("===============================================")
	fmt.Println(cleanedText)
	fmt.Println("===============================================")

	return nil
}
